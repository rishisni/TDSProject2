# import os
# import sys
# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt
# import seaborn as sns
# import openai
# from dotenv import load_dotenv

# # Load environment variables
# load_dotenv()

# # Set OpenAI API configurations
# API_BASE = "https://aiproxy.sanand.workers.dev/openai/v1"
# API_KEY = os.environ.get("AIPROXY_TOKEN")
# MODEL = "gpt-4o-mini"

# # Configure OpenAI API client
# openai.api_base = API_BASE
# openai.api_key = API_KEY

# # Validate OpenAI API Key
# if not openai.api_key:
#     print("Error: AIPROXY_TOKEN is not set. Ensure the token is loaded in the environment.")
#     sys.exit(1)

# # Function to call the LLM
# def get_llm_response(prompt):
#     try:
#         print("Sending request to LLM...")
#         response = openai.ChatCompletion.create(
#             model=MODEL,
#             messages=[
#                 {"role": "system", "content": "You are an AI analyst."},
#                 {"role": "user", "content": prompt}
#             ],
#             max_tokens=800
#         )
#         print("LLM Raw Response:", response)
#         if "choices" in response and len(response.choices) > 0:
#             return response.choices[0].message["content"].strip()
#         else:
#             return "No analysis provided by LLM."
#     except Exception as e:
#         print(f"Error with LLM: {e}")
#         return "LLM analysis failed."

# # Validate command-line arguments
# if len(sys.argv) != 2:
#     print("Usage: python autolysis.py <dataset.csv>")
#     sys.exit(1)

# # Load the dataset
# csv_file = sys.argv[1]
# if not os.path.exists(csv_file):
#     print(f"File {csv_file} not found.")
#     sys.exit(1)

# data = pd.read_csv(csv_file, encoding='ISO-8859-1')

# # Generate summary statistics
# summary_stats = data.describe(include="all").transpose()
# missing_values = data.isnull().sum()
# correlation_matrix = data.corr(numeric_only=True)

# # Define the output directory based on the dataset name
# dataset_name = os.path.splitext(os.path.basename(csv_file))[0]
# output_dir = os.path.join(dataset_name)

# # Create the output directory if it doesn't exist
# os.makedirs(output_dir, exist_ok=True)

# # Save correlation matrix heatmap
# plt.figure(figsize=(10, 8))
# sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="coolwarm", cbar=True)
# plt.title("Correlation Matrix")
# plt.savefig(os.path.join(output_dir, "correlation_matrix.png"))
# plt.close()

# # Generate distribution plots for numerical columns
# for column in data.select_dtypes(include=[np.number]).columns:
#     plt.figure(figsize=(8, 6))
#     sns.histplot(data[column].dropna(), kde=True, bins=30, color="blue")
#     plt.title(f"Distribution of {column}")
#     plt.xlabel(column)
#     plt.ylabel("Frequency")
#     plt.savefig(os.path.join(output_dir, f"{column}_distribution.png"))
#     plt.close()

# # Prepare data for LLM
# sample_data = data.head(5).to_dict(orient="records")
# llm_prompt = f"""
# You are an AI analyst. Here's the dataset summary:
# - Columns: {list(data.columns)}
# - Data types: {data.dtypes.to_dict()}
# - Missing values: {missing_values.to_dict()}
# - Sample data: {sample_data}

# Perform the following:
# 1. Analyze the data and provide insights.
# 2. Suggest any interesting trends, outliers, or relationships.

# Be concise and professional.
# """

# llm_analysis = get_llm_response(llm_prompt)

# # Generate README.md content
# markdown_content = f"""
# # Automated Analysis Report

# ## Dataset Overview
# - **Rows**: {data.shape[0]}
# - **Columns**: {data.shape[1]}
# - **Missing Values**:
# {missing_values.to_string()}

# ## Key Insights
# {llm_analysis}

# ## Visualizations
# ### Correlation Matrix
# ![Correlation Matrix](correlation_matrix.png)

# ### Distributions
# """

# # Add distribution plots to the README content
# for column in data.select_dtypes(include=[np.number]).columns:
#     markdown_content += f"![{column} Distribution]({column}_distribution.png)\n"

# # Save README.md inside the output directory
# readme_path = os.path.join(output_dir, "README.md")
# with open(readme_path, "w") as f:
#     f.write(markdown_content)

# print(f"Analysis complete. Outputs saved in {output_dir}/README.md and PNG files.")


import os
import sys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import openai
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Set OpenAI API configurations
API_BASE = "https://aiproxy.sanand.workers.dev/openai/v1"
API_KEY = os.environ.get("AIPROXY_TOKEN")
MODEL = "gpt-4o-mini"

# Configure OpenAI API client
openai.api_base = API_BASE
openai.api_key = API_KEY

# Validate OpenAI API Key
if not openai.api_key:
    print("Error: AIPROXY_TOKEN is not set. Ensure the token is loaded in the environment.")
    sys.exit(1)

# Function to call the LLM
def get_llm_response(prompt):
    try:
        print("Sending request to LLM...")
        response = openai.ChatCompletion.create(
            model=MODEL,
            messages=[
                {"role": "system", "content": "You are an AI analyst."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=800
        )
        print("LLM Raw Response:", response)
        if "choices" in response and len(response.choices) > 0:
            return response.choices[0].message["content"].strip()
        else:
            return "No analysis provided by LLM."
    except Exception as e:
        print(f"Error with LLM: {e}")
        return "LLM analysis failed."

# Validate command-line arguments
if len(sys.argv) != 2:
    print("Usage: python autolysis.py <dataset.csv>")
    sys.exit(1)

# Load the dataset
csv_file = sys.argv[1]
if not os.path.exists(csv_file):
    print(f"File {csv_file} not found.")
    sys.exit(1)

try:
    data = pd.read_csv(csv_file, encoding='ISO-8859-1')
except Exception as e:
    print(f"Error loading dataset: {e}")
    sys.exit(1)

# Generate summary statistics
summary_stats = data.describe(include="all").transpose()
missing_values = data.isnull().sum()
correlation_matrix = data.corr(numeric_only=True)

# Define the output directory based on the dataset name
dataset_name = os.path.splitext(os.path.basename(csv_file))[0]
output_dir = os.path.join(dataset_name)

# Create the output directory if it doesn't exist
os.makedirs(output_dir, exist_ok=True)

# Save correlation matrix heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="coolwarm", cbar=True)
plt.title("Correlation Matrix")
plt.savefig(os.path.join(output_dir, "correlation_matrix.png"))
plt.close()

# Generate distribution plots for numerical columns
for column in data.select_dtypes(include=[np.number]).columns:
    plt.figure(figsize=(8, 6))
    sns.histplot(data[column].dropna(), kde=True, bins=30, color="blue")
    plt.title(f"Distribution of {column}")
    plt.xlabel(column)
    plt.ylabel("Frequency")
    plt.savefig(os.path.join(output_dir, f"{column}_distribution.png"))
    plt.close()

# Prepare data for LLM
sample_data = data.head(5).to_dict(orient="records")
llm_prompt = f"""
You are an AI data analyst. Here's the dataset summary:
- Number of Rows: {data.shape[0]}
- Number of Columns: {data.shape[1]}
- Columns: {list(data.columns)}
- Data Types: {data.dtypes.to_dict()}
- Missing Values: {missing_values.to_dict()}
- Sample Data: {sample_data}

Your task:
1. Analyze the dataset.
2. Highlight important trends, outliers, and correlations.
3. Suggest potential applications or interpretations of the data.
4. Provide actionable insights.

Be concise and professional.
"""

llm_analysis = get_llm_response(llm_prompt)

# Generate README.md content
markdown_content = f"""
# Automated Analysis Report

## Dataset Overview
- **Rows**: {data.shape[0]}
- **Columns**: {data.shape[1]}
- **Missing Values**:
{missing_values.to_string()}

## Key Insights
{llm_analysis}

## Visualizations
### Correlation Matrix
![Correlation Matrix](correlation_matrix.png)

### Distributions
"""

# Add distribution plots to the README content
for column in data.select_dtypes(include=[np.number]).columns:
    markdown_content += f"![{column} Distribution]({column}_distribution.png)\n"

# Save README.md inside the output directory
readme_path = os.path.join(output_dir, "README.md")
with open(readme_path, "w") as f:
    f.write(markdown_content)

print(f"Analysis complete. Outputs saved in {output_dir}/README.md and PNG files.")
